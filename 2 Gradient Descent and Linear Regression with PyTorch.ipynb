{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and Linear Regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "\n",
    "In this tutorial, we'll discuss one of the foundational algorithms in machine learning: Linear regression. We'll create a model that predicts crop yields for apples and oranges (target variables) by looking at the average temperature, rainfall, and humidity (input variables or features) in a region. Here's the training data:\n",
    "\n",
    "![table](https://i.imgur.com/6Ujttb4.png)\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "yield_apple  = w11 * temp + w12 * rainfall + w13 * humidity + b1\n",
    "\n",
    "yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "![linear-regression-graph](https://i.imgur.com/4DJ9f8X.png)\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights w11, w12,... w23, b1 & b2 using the training data, to make accurate predictions for new data. The learned weights will be used to predict the yields for apples and oranges in a new region using the average temperature, rainfall, and humidity for that region.\n",
    "\n",
    "We'll train our model by adjusting the weights slightly many times to make better predictions, using an optimization technique called gradient descent. Let's begin by importing Numpy and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "np.shape(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "np.shape(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Converting inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1095,  0.9673,  0.6344],\n",
      "        [ 1.2768,  1.1176, -1.6715]], requires_grad=True)\n",
      "tensor([-0.9188, -1.1990], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and Biases\n",
    "w = torch.randn(2, 3, requires_grad = True)\n",
    "b = torch.randn(2, requires_grad = True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn creates a tensor with the given shape, with elements picked randomly from a normal distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "Our model is simply a function that performs a matrix multiplication of the inputs and the weights w (transposed) and adds the bias b (replicated for each observation).\n",
    "\n",
    "![linear_regression](https://i.imgur.com/WGXLFvA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@ reprents matrix multiplication in PyTorch, and the .t method returns the transpose of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -62.8211,   95.0054],\n",
      "        [ -67.1555,  106.3539],\n",
      "        [ -18.0296,  162.6839],\n",
      "        [-151.0166,  115.2388],\n",
      "        [  -9.2026,   77.1768]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets \n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1-t2\n",
    "    return torch.sum(diff*diff) / diff.numel()\n",
    "\n",
    "# torch.sum(input, *, dtype=None) → Tensor\n",
    "# Returns the sum of all elements in the input tensor.\n",
    "# https://pytorch.org/docs/stable/generated/torch.sum.html\n",
    "\n",
    "# torch.numel(input) → int\n",
    "# Returns the total number of elements in the input tensor.\n",
    "# https://pytorch.org/docs/stable/generated/torch.numel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10677.5312, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Computing the Loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Gradients\n",
    "\n",
    "# With PyTorch, we can automatically compute the gradient or derivative of the \n",
    "# loss w.r.t. to the weights and biases, because they have requires_grad set to \n",
    "# True.\n",
    "\n",
    "# The gradients are stored in the .grad property of the respective tensors. Note \n",
    "# that the derivative of the loss w.r.t. the weights matrix is itself a matrix, \n",
    "# with the same dimensions.\n",
    "\n",
    "# To reduce memory usage, during the .backward() call, all the intermediary \n",
    "# results are deleted when they are not needed anymore. Hence if you try to call \n",
    "# .backward() again, the intermediary results don’t exist and the backward pass \n",
    "# cannot be performed (and you get the error you see). You can call \n",
    "# .backward(retain_graph=True) to make a backward pass that will not delete \n",
    "# intermediary results, and so you will be able to call .backward() again. All \n",
    "# but the last call to backward should have the retain_graph=True option\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1095,  0.9673,  0.6344],\n",
      "        [ 1.2768,  1.1176, -1.6715]], requires_grad=True)\n",
      "tensor([[-11893.4668, -11514.3643,  -7358.9541],\n",
      "        [  1997.9314,   1094.6770,    621.3517]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for the weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9188, -1.1990], requires_grad=True)\n",
      "tensor([-137.8451,   19.2918])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting weights and biases to reduce the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the no grad as we dont want the computational graph to include\n",
    "# the gradient in its own graph calculation\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -43.2585,   92.5461],\n",
      "        [ -41.4887,  103.1746],\n",
      "        [  12.0166,  159.1182],\n",
      "        [-131.2099,  112.5001],\n",
      "        [  15.2103,   74.3122]], grad_fn=<AddBackward0>)\n",
      "tensor(7637.3960, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Verifying if the loss is lower\n",
    "\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Before we proceed, we reset the gradients to zero by calling .zero_() method. \n",
    "# We need to do this, because PyTorch accumulates, gradients i.e. the next time \n",
    "# we call .backward on the loss, the new gradient values will get added to the \n",
    "# existing gradient values, which may lead to unexpected results.\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model using Gradient Descent\n",
    "\n",
    "As seen above, we reduce the loss and improve our model using the gradient descent optimization algorithm. Thus, we can train the model using the following steps:\n",
    "\n",
    " - Generate predictions\n",
    "\n",
    " - Calculate the loss\n",
    "\n",
    " - Compute gradients w.r.t the weights and biases\n",
    "\n",
    " - Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    " - Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs \n",
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(592.3265, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculating Losses\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 51.1445,  77.0389],\n",
       "        [ 79.6443,  87.1033],\n",
       "        [134.1974, 152.9909],\n",
       "        [-12.8761,  75.4734],\n",
       "        [117.0995,  72.8562]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using PyTorch Built-Ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by importing the torch.nn package from PyTorch, which \n",
    "# contains utility classes for building neural networks.\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a TensorDataset, which allows access to rows from \n",
    "# inputs and targets as tuples, and provides standard APIs for working with \n",
    "# many different types of datasets in PyTorch.\n",
    "\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the Dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Dataloader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  88.,  64.],\n",
      "        [101.,  44.,  37.],\n",
      "        [103.,  43.,  36.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [ 68.,  97.,  70.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [ 21.,  38.],\n",
      "        [ 20.,  38.],\n",
      "        [118., 134.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, the data loader returns one batch of data with the given batch size. If shuffle is set to True, it shuffles the training data before creating batches. Shuffling helps randomize the input to the optimization algorithm, leading to a faster reduction in the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear\n",
    "\n",
    "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically.\n",
    "\n",
    "Linear\n",
    "module = Linear(inputDimension,outputDimension)\n",
    "\n",
    "Applies a linear transformation to the incoming data, i.e. //y= Ax+b//. The input tensor given in forward(input) must be either a vector (1D tensor) or matrix (2D tensor). If the input is a matrix, then each row is assumed to be an input sample of given batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2883, -0.4666, -0.1476],\n",
      "        [ 0.2662,  0.4421, -0.1961]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2754, 0.1876], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "print(model.weight.shape)\n",
    "print(model.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.2883, -0.4666, -0.1476],\n",
       "         [ 0.2662,  0.4421, -0.1961]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.2754, 0.1876], requires_grad=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-16.2863,  40.8119],\n",
       "        [-23.9941,  50.7704],\n",
       "        [-45.7243,  71.2187],\n",
       "        [  4.1567,  39.0990],\n",
       "        [-34.9540,  47.2734],\n",
       "        [-15.5314,  40.6361],\n",
       "        [-23.6751,  50.1322],\n",
       "        [-45.5836,  71.2888],\n",
       "        [  3.4018,  39.2749],\n",
       "        [-35.3898,  46.8110],\n",
       "        [-15.9673,  40.1737],\n",
       "        [-23.2393,  50.5945],\n",
       "        [-46.0434,  71.8569],\n",
       "        [  4.5925,  39.5613],\n",
       "        [-35.7088,  47.4492]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds.shape)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "Instead of defining a loss function manually, we can use the built-in loss function mse_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7491.4233, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Instead of manually manipulating the model's weights & biases using gradients, we can use the optimizer optim.SGD. SGD is short for \"stochastic gradient descent\". The term stochastic indicates that samples are selected in random batches instead of as a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate that controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "We are now ready to train the model. We'll follow the same process to implement gradient descent:\n",
    "\n",
    " - Generate predictions\n",
    "\n",
    " - Calculate the loss\n",
    "\n",
    " - Compute gradients w.r.t the weights and biases\n",
    "\n",
    " - Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "\n",
    " - Reset the gradients to zero\n",
    "\n",
    "The only change is that we'll work batches of data instead of processing the entire training data in every iteration. Let's define a utility function fit that trains the model for a given number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with the batches of data\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            # 1. Generate Predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate Loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Computing the Gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update the parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # Printing the progress\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to note above:\n",
    "\n",
    " - We use the data loader defined earlier to get batches of data for every iteration.\n",
    "\n",
    " - Instead of updating parameters (weights and biases) manually, we use opt.step to perform the update and opt.zero_grad to reset the gradients to zero.\n",
    "\n",
    " - We've also added a log statement that prints the loss from the last batch of data for every 10th epoch to track training progress. loss.item returns the actual value stored in the loss tensor.\n",
    "\n",
    "Let's train the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 757.1930\n",
      "Epoch [10/100], Loss: 593.8428\n",
      "Epoch [10/100], Loss: 598.0179\n",
      "Epoch [20/100], Loss: 461.1000\n",
      "Epoch [20/100], Loss: 413.1428\n",
      "Epoch [20/100], Loss: 475.2563\n",
      "Epoch [30/100], Loss: 435.5168\n",
      "Epoch [30/100], Loss: 317.0444\n",
      "Epoch [30/100], Loss: 253.0004\n",
      "Epoch [40/100], Loss: 27.9852\n",
      "Epoch [40/100], Loss: 379.7800\n",
      "Epoch [40/100], Loss: 282.8146\n",
      "Epoch [50/100], Loss: 171.4773\n",
      "Epoch [50/100], Loss: 114.6867\n",
      "Epoch [50/100], Loss: 233.7007\n",
      "Epoch [60/100], Loss: 59.1532\n",
      "Epoch [60/100], Loss: 198.0815\n",
      "Epoch [60/100], Loss: 118.8027\n",
      "Epoch [70/100], Loss: 64.1531\n",
      "Epoch [70/100], Loss: 103.1029\n",
      "Epoch [70/100], Loss: 122.5319\n",
      "Epoch [80/100], Loss: 77.6288\n",
      "Epoch [80/100], Loss: 82.1988\n",
      "Epoch [80/100], Loss: 77.7008\n",
      "Epoch [90/100], Loss: 54.8107\n",
      "Epoch [90/100], Loss: 42.5772\n",
      "Epoch [90/100], Loss: 85.4586\n",
      "Epoch [100/100], Loss: 62.1815\n",
      "Epoch [100/100], Loss: 59.2382\n",
      "Epoch [100/100], Loss: 31.9695\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate predictions using our model and verify that they're close to our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.8197,  71.7688],\n",
       "        [ 80.9237,  96.1433],\n",
       "        [117.3367, 140.3239],\n",
       "        [ 31.4714,  45.2254],\n",
       "        [ 93.7960, 106.5507],\n",
       "        [ 57.8133,  70.7297],\n",
       "        [ 80.4798,  95.3810],\n",
       "        [117.5378, 140.4921],\n",
       "        [ 32.4779,  46.2644],\n",
       "        [ 94.3586, 106.8275],\n",
       "        [ 58.3758,  71.0065],\n",
       "        [ 79.9173,  95.1042],\n",
       "        [117.7805, 141.0862],\n",
       "        [ 30.9089,  44.9486],\n",
       "        [ 94.8024, 107.5897]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparing it with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the predictions are quite close to our targets. We have a trained a reasonably good model to predict crop yields for apples and oranges by looking at the average temperature, rainfall, and humidity in a region. We can use it to make predictions of crop yields for new regions by passing a batch containing a single row of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55.5373, 67.9437]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted yield of apples is 54.3 tons per hectare, and that of oranges is 68.3 tons per hectare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning vs. Classical Programming\n",
    "\n",
    "The approach we've taken in this tutorial is very different from programming as you might know it. Usually, we write programs that take some inputs, perform some operations, and return a result.\n",
    "\n",
    "However, in this notebook, we've defined a \"model\" that assumes a specific relationship between the inputs and the outputs, expressed using some unknown parameters (weights & biases). We then show the model some know inputs and outputs and train the model to come up with good values for the unknown parameters. Once trained, the model can be used to compute the outputs for new inputs.\n",
    "\n",
    "This paradigm of programming is known as machine learning, where we use data to figure out the relationship between inputs and outputs. Deep learning is a branch of machine learning that uses matrix operations, non-linear activation functions and gradient descent to build and train models. Andrej Karpathy, the director of AI at Tesla Motors, has written a great blog post on this topics, titled [Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35).\n",
    "\n",
    "This picture from book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python) by Francois Chollet captures the difference between classical programming and machine learning:\n",
    "\n",
    "![image](https://i.imgur.com/oJEQe7k.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}